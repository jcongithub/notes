Hadoop - a framwork for reliable, scalable distributed computing
  1. using simple programming models for the distributed processing of large data sets across clusters of computers
  2. scale up from single servers to thousands of machines
  3. rather than rely on hardware to deliver high-availiability, the framwork itself is designed to detect and handle 
     failures at the application layer, so delivering a highly-available service on top of a cluster of computers

  
